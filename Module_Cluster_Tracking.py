# -*- coding: utf-8 -*-
"""Thesis_Tajeuna

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uu4o4z5_dVmupSs4jcPGXFdDi1IeJihR
"""

import time
import csv
import requests
import tqdm
import math
import json
import sys, setuptools, tokenize
import community as community_louvain
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from datetime import datetime, timedelta, date
from dateutil.relativedelta import relativedelta
from itertools import permutations 
from scipy.optimize import linear_sum_assignment
from collections import OrderedDict
from sklearn.cluster import KMeans

def read_graph_new (node_df, edge_df, window_length, start_date):
  end_date = start_date +  relativedelta(months =+ window_length*3)
  end_time = math.ceil(end_date.month/window_length)

  nodes_df = pd.DataFrame(columns=['Occurance', 'Node', 'Time', 'Country'])
  edges_df = pd.DataFrame(columns=['Coccurance', 'Node1', 'Node2', 'Time', 'Country'])
  
  for index, row in node_df.iterrows():
    given_date = datetime.strptime(row['Time'].strip()[0:10], '%Y-%m-%d')
    time = math.ceil(given_date.month/window_length)
    if start_date <= given_date < end_date:
      nodes_df = nodes_df.append({'Occurance': row['Occurance'], 'Node': row['Node'], 'Time': time, 'Country': row['Country']}, ignore_index=True)
  for index, row in edge_df.iterrows():
    given_date = datetime.strptime(row['Time'].strip()[0:10], '%Y-%m-%d')
    time = math.ceil(given_date.month/window_length)
    if start_date <= given_date < end_date:
      edges_df = edges_df.append({'Coccurance': row['Coccurance'], 'Node1': row['Node1'], 'Node2' : row['Node2'], 'Time': time, 'Country': row['Country']}, ignore_index=True)
  G = read_graph(nodes_df, edges_df)
  return G

def read_graph (nodes_df, edges_df):
  G = nx.Graph()
  for index, row in nodes_df.iterrows():
    if G.has_node(row['Node']):
      G.nodes[row['Node']]['attr_dict']['occurance'] = G.nodes[row['Node']]['attr_dict']['occurance'] + row['Occurance']
    else:
      G.add_node(row['Node'], attr_dict = {'time':row['Time'], 'country':row['Country'], 'occurance':row['Occurance']})

  for index, row in edges_df.iterrows():
    if G.has_edge(row['Node1'], row['Node2']):
      G.edges[row['Node1'], row['Node2']]['attr_dict']['Coccurance'] = G.edges[row['Node1'], row['Node2']]['attr_dict']['Coccurance'] + row['Coccurance']
    else:
      G.add_edge(row['Node1'], row['Node2'], attr_dict = {'Coccurance':row['Coccurance']})
  return G

def read_from_path(node_path, edge_path):
  node_df = pd.read_csv(node_path, sep=";")
  edge_df = pd.read_csv(edge_path, sep=";")
  return node_df, edge_df

def get_master_graph(node_path, edge_path):
  node_df, edge_df = read_from_path (node_path, edge_path)
  G = nx.Graph()
  G = read_graph (node_df, edge_df)
  return G

def get_communities (G):
  partition = community_louvain.best_partition(G)
  community_values = set(partition.values())
  communities = {}
  for n in community_values:
    communities[n] = [k for k in partition.keys() if partition[k] == n]
  return communities, partition

def remove_outliners (community):
  updated = {}
  updated.update(community)
  for head, body in community.items():
    if len(body) == 1:
      updated.pop(head)
  return updated

def get_number_communities(read_dictionary):
  total_count = []
  for head, body in read_dictionary.items():
    for h , b in body.items():
      total_count.append(len(b))
  return total_count

def create_dataframe (G_master, total_count):
  rows = []
  columns = []
  for node in G_master.nodes():
    rows.append(node)
  time = 1
  for numbers in total_count:
    for i in range(numbers):
      columns.append('C'+str(time)+str(i+1))
    time = time + 1
  df = pd.DataFrame(index = rows, columns = columns)
  df = df.fillna(0)
  return df, rows, columns

def construct_membership_matix(df, data_dict, rows):
  for h, v in data_dict.items():
    new_dict = v['de']
    time = h+1
    for head, values in new_dict.items():
      column = 'C'+str(time)+str(head+1)
      #df_column = pd.DataFrame(index = rows, columns = 'C'+str(head)+str(i))
      for n in values:
        df.at[n, column] = 1
  return df.to_numpy()

def get_burt_matrix(df_numpy):
  matrix_A = np.matrix(df_numpy, dtype=None, copy=True)
  A_transpose = matrix_A.transpose()
  burt_matrix = A_transpose.dot(matrix_A)
  return burt_matrix

def normalize_burt (burt, columns):
  number_of_rows, number_of_columns = burt.shape
  probability_matrix = np.zeros(shape=[number_of_rows, number_of_columns])
  sum_matrix = burt.sum(axis=1)
  for row in range(number_of_rows):
    sum = sum_matrix[row]
    for column in range(number_of_columns):
      value = burt[row, column]
      probability_matrix[row, column] = value/sum
  df_probability = pd.DataFrame(data=probability_matrix, index=columns, columns=columns)
  return df_probability

def calculate_mean (p1, p2):
  if p1 == 0.0 or p2 == 0.0:
    return 0
  else:
    mean = 2*((p1*p2)/(p1+p2))
    return mean

def get_transition_probability (probability_df):
  transition_probability = {}
  for row1 in df_probability.columns:
    for row2 in df_probability.columns:
      sum = 0
      if row1 == row2:
        continue
      if int(row2[1:2]) == int(row1[1:2])+1:
        v1 = df_probability.loc[[row1]].values.tolist()[0]
        v2 = df_probability.loc[[row2]].values.tolist()[0]
        for i in range(len(v1)):
          value = calculate_mean(v1[i], v2[i])
          sum = sum + value
        transition_probability[row1+row2] = sum
  return transition_probability

def get_threshold_value(transition_probability):
  sorted_d = sorted(transition_probability.items(), key=operator.itemgetter(1))
  Ics = []
  for item in sorted_d:
    Ics.append(item[1])
  npArray = np.array(Ics , dtype=float) 
  npArray = npArray.reshape(-1, 1)
  kmeans = KMeans(n_clusters=2, init='k-means++').fit(npArray)
  center1 = kmeans.cluster_centers_[0]
  center2 = kmeans.cluster_centers_[1]
  threshold = (center1+center2)/2 
  return threshold

def get_jaccard_threshold(df_probability):
  jaccard_values = {}
  for row1 in df_probability.columns:
    for row2 in df_probability.columns:
      if row1 == row2:
        continue
      else:
        value = get_jaccard_index(row1, row2, df_probability)
        jaccard_values[row1+row2] = value
  sorted_jaccard = sorted(jaccard_values.items(), key=operator.itemgetter(1))
  jcs = []
  for item in sorted_jaccard:
    jcs.append(item[1])
  npArray = np.array(jcs , dtype=float) 
  npArray = npArray.reshape(-1, 1)
  kmeans = KMeans(n_clusters=2, init='k-means++').fit(npArray)
  center1 = kmeans.cluster_centers_[0]
  center2 = kmeans.cluster_centers_[1]
  threshold_jaccard = (center1+center2)/2 
  return threshold_jaccard

def get_jaccard_index(cluster1, cluster2, df_probability):
  value1 = df_probability.at[cluster1, cluster2]
  value2 = df_probability.at[cluster1, cluster1]
  jaccard_index = ((value1*value2)/(value2+value1))
  return jaccard_index

def get_timewise_clusters(df_probability):
  timewise_clusters = {}
  for row1 in df_probability.columns:
    cluster = []
    for row2 in df_probability.columns:
      if row1[1:2] == row2[1:2]:
        cluster.append(row2)
      else:
        continue
    timewise_clusters[row1[1:2]] = cluster
  return timewise_clusters

def track_communities(threshold, timewise_clusters, df_probability, transition_probability, threshold_jaccard):
  evolution_dict = {}
  for head, value in timewise_clusters.items():
    sets = {}
    next_slot = int(head)+1
    for cluster in value:
      new_head = 0
      if next_slot == 5: #Change this value
        break
      for next_value in timewise_clusters[str(next_slot)]:
        jaccard_index = get_jaccard_index(cluster, next_value, df_probability)
        if transition_probability[cluster+next_value] > threshold[0] and jaccard_index > threshold_jaccard[0]:
          sets[cluster] = next_value
    evolution_dict[head] = sets
  return evolution_dict

def polish_evolution_dict(evolution_dict):
  new_one = {}
  for head, value in evolution_dict.items():
    adding = {}
    for present, next in value.items():
      lists = []
      for p1, n1 in value.items():
        if n1 == next:
          lists.append(p1)
      adding[next] = lists
    new_one[head] = adding
  return new_one

def prepare_cluster_list(polished_dict, community_dict, total_columns, country, total_count):
  complete_dict = {}
  for column in total_columns:
    complete_dict[column] = community_dict[int(column[1:2])-1][country][int(column[1:2])-1]
  cluster_evolution_dict = {}
  cluster = {}
  for head, value in polished_dict.items():
    evolution = {}
    if int(head) == 1:
      for h, v in value.items():
        cluster_value = round(random.uniform(1, total_count[int(head)-1]), 4)
        evolution[h] = cluster_value
        cluster[h] = cluster_value
        for vs in v:
          evolution[vs] = cluster_value
          cluster[vs] = cluster_value
      cluster_evolution_dict[head] = evolution
    else:
      for h, v in value.items():
        cluster_value =  round(random.uniform(1, total_count[int(head)-1]), 4)
        for vs in v:
          #print(cluster_evolution_dict[str(int(head)-1)].keys())
          if(vs in cluster_evolution_dict[str(int(head)-1)].keys()):
            cluster_value = cluster_evolution_dict[str(int(head)-1)][vs]
          evolution[vs] = cluster_value
          cluster[vs] = cluster_value
        evolution[h] = cluster_value
        cluster[h] = cluster_value
        cluster_evolution_dict[head] = evolution
  return complete_dict, cluster

def get_total_number_clusters(total_count):
  time = 1
  number_of_clusters = {}
  for numbers in total_count:
    lists = []
    for i in range(numbers):
      lists.append('C'+str(time)+str(i+1))
    number_of_clusters[time] = lists
    time = time + 1
  return number_of_clusters

def calculate_burt_kg(probability_df, community_dict, country):
  for row1 in df_probability.columns:
    for row2 in df_probability.columns:
      sum = 0
      if row1 == row2:
        continue
      if int(row2[1:2]) == int(row1[1:2])+1:
        list1 = community_dict[int(row1[1:2])-1][country][int(row1[2:])-1]
        list2 = community_dict[int(row2[1:2])-1][country][int(row2[2:])-1]
        if len(list1) == 1 or len(list2) == 1:
          print('Continue as length is 1')
          continue
        print(row1+ '&' + row2)
        probability_kg = get_weight_from_KG(list1, list2)
        burt_value = probability_df.loc[row1][row2]
        new_burt_value = burt_value+probability_kg
        probability_df.loc[row1][row2] = new_burt_value
  return probability_df

def get_weight_from_KG(list1, list2):
  checked_list1 = []
  checked_list2 = []
  checked_list1 = map_skills(list1)
  checked_list2 = map_skills(list2)
  print(checked_list1)
  print(checked_list2)
  t1_triples_df = getDbpediaTriplesOfCluster(checked_list1)
  t2_triples_df = getDbpediaTriplesOfCluster(checked_list2)
  min_cluster_size = min(len(checked_list1), len(checked_list2))
  sim_score = mapClusters(checked_list1, checked_list2, t1_triples_df, t2_triples_df)
  norm_sim_score = sim_score / min_cluster_size
  print(norm_sim_score)
  return norm_sim_score

def normalize_names(node):
  node = node.capitalize().strip()

  if node.find('-') != -1:
    node = node[0:node.find('-')] + '_'+ node[node.find('-')+1:]
  if node == 'Jquery':
    node = 'JQuery'
  if node == 'Xhtml':
    node = 'XHTML'
  if node == 'Php':
    node = 'PHP'
  if node == 'Angularjs':
    node = 'AngularJS'
  if node == 'Html5':
    node = 'HTML5'
  
  return node
def map_skills(skills_to_be_mapped):
# skills_to_be_mapped = ['mysql','oracle','postgresql','database']
  cluster = []
  for skill in skills_to_be_mapped:
    link = annotate_with_Dbpedia_spotlight(skill, 0.5)
    if link == 0:
      confidence = 0.4
      while confidence >= 0:
        link = annotate_with_Dbpedia_spotlight(skill, confidence)
        confidence = confidence - 0.1
    if link == 0:
      skill = normalize_names(skill)
      link = 'http://dbpedia.org/resource/'+skill
    cluster.append(link)
  return cluster


def annotate_with_Dbpedia_spotlight(text, confidence):
  # text preprocessing
  text = text.replace("_", " ").replace("-", " ")
  URL = "https://api.dbpedia-spotlight.org/en/annotate?text=" + text + "&confidence=" + str(confidence) + ""
  HEADERS = {'Accept': 'application/json'}
  response = requests.get(URL, headers=HEADERS)
  if response.status_code != 200:
    return 0
  
  json_obj = response.json()
  if "Resources" in json_obj:
    return json_obj["Resources"][0]['@URI']
  else:
    return 0

def getTriples(A): 
  url = 'http://dbpedia.org/sparql/'
  query = """
  SELECT *
  WHERE
  {
    {
    <""" + A + """>  ?r1 ?n2 .
    }
  }
  """
  r = requests.get(url, params = {'format': 'json', 'query': query})
  data = r.json()

  subgraph = []
  for item in data['results']['bindings']:
      if item['n2']['value'].startswith('http://dbpedia.org'):
        subgraph.append(OrderedDict({
          'source_node': A, 
          'r1': item['r1']['value'],
          'target_node': item['n2']['value']
        }))

  df = pd.DataFrame(subgraph)
  return df

def getConnectingRelation(node):
  url = 'http://dbpedia.org/sparql/'
  query = """
  SELECT *
  WHERE
  {
    {
    <""" + node + """>  ?r1 ?n2 .
    }
  }
  """
  r = requests.get(url, params = {'format': 'json', 'query': query})
  data = r.json()

  relations = []
  for item in data['results']['bindings']:
    relations.append(item['r1']['value'])

  relations = list(set(relations))
  return relations

def getDbpediaTriplesOfCluster(cluster):
  frames = []
  for item in cluster:
    df = getTriples(item)
    frames.append(df)
  result = pd.concat(frames)
  result = result.drop_duplicates()
  return result

def firstRuleOfSimilarity(s1, s2, s1_triples, s2_triples):
  # If two distinct subjects share the same predicate, and for that predicate the same object, then both are given weight as 'similar'
  df1 = s1_triples.query("source_node == '" + s1 + "'")
  df2 = s2_triples.query("source_node == '" + s2 + "'")
  score = 0
  # find common items in df1 and df2
  df = df1.merge(df2, how = 'inner' ,indicator=False)
  
  if not df.empty:
    unique_predicates = list(df.r1.unique())
    for pred in unique_predicates:
      common_rows = df.query("r1 == '" + pred + "'").shape[0]
      df1_pred_rows = df1.query("r1 == '" + pred + "'").shape[0]
      df2_pred_rows = df2.query("r1 == '" + pred + "'").shape[0]
      score = score + (common_rows / (df1_pred_rows + df2_pred_rows - common_rows))
  return score

def secondRuleOfSimilarity(s1, s2, s1_triples, s2_triples):
  # If two distinct subjects have similar direct neighbor nodes, 
  # then they are considered similar (for this we can give a threshold for the number of direct neighbor nodes that are similar)
  node_list1 = s1_triples.query("source_node == '" + s1 + "'")["target_node"].tolist()
  node_list2 = s2_triples.query("source_node == '" + s2 + "'")["target_node"].tolist()
  node_set1 = set(node_list1)
  common_items = list(node_set1.intersection(node_list2))
  if len(common_items) > 0:
    result = len(common_items) / (len(node_list1) + len(node_list2) - len(common_items))
    return result
  else:
    return 0

def thirdRuleOfSimilarity(s1, s2, s1_triples, s2_triples):
  excluded_predicates_list = ['http://www.w3.org/1999/02/22-rdf-syntax-ns#type']
  relation_list1 = s1_triples.query("source_node == '" + s1 + "'")["r1"].tolist()
  relation_list2 = s2_triples.query("source_node == '" + s2 + "'")["r1"].tolist()
  relation_set1 = set(relation_list1)
  common_relations = list(relation_set1.intersection(relation_list2))
  common_relations = list(set(common_relations) - set(excluded_predicates_list))
  # print(common_relations)
  if len(common_relations) > 0:
    result = len(common_relations) / (len(relation_list1) + len(relation_list2) - len(common_relations))
    return result
  else:
   return 0
  
def mapClusters(t1_cluster, t2_cluster, t1_triples_df, t2_triples_df):
  score_matrix = []
  for t1_item in t1_cluster:
    row_list = []
    for t2_item in t2_cluster:
      if t1_item != t2_item:
        first_sim_score = firstRuleOfSimilarity(t1_item, t2_item, t1_triples_df, t2_triples_df)
        second_sim_score = secondRuleOfSimilarity(t1_item, t2_item, t1_triples_df, t2_triples_df)
        third_sim_score = thirdRuleOfSimilarity(t1_item, t2_item, t1_triples_df, t2_triples_df)
        total_sim_score = float("{:.4f}".format((first_sim_score + second_sim_score + third_sim_score)/3))
      else:
        total_sim_score = 1
      row_list.append(total_sim_score)
    score_matrix.append(row_list)
  score_matrix = np.array(score_matrix)
  # applying hungarian algo to get max similarity score
  row_ind, col_ind = linear_sum_assignment(-score_matrix)
  total_similarity = score_matrix[row_ind, col_ind].sum()
  return total_similarity

#G_master = get_master_graph('/content/drive/My Drive/dataset/PeiLee/nodes_de.csv', '/content/drive/My Drive/dataset/PeiLee/de.csv')
community_dict = np.load('/content/drive/MyDrive/Thesis/Tajuena_clusters_1617_new.npy',allow_pickle='TRUE').item()
total_count = get_number_communities(community_dict)
print(total_count)
df, rows, columns = create_dataframe(G_master, total_count)
df_numpy = construct_membership_matix(df, community_dict, rows)
print(df.shape)
burt = get_burt_matrix(df_numpy)
df_probability = normalize_burt(burt, columns)
print('Calculating KG similarity')
df_probability = calculate_burt_kg(df_probability, community_dict, 'de')
df_probability.to_pickle('/content/drive/My Drive/Thesis/probabilitydf_new1617.pkl')
transition_probability = get_transition_probability(df_probability)
threshold = get_threshold_value(transition_probability)
threshold_jaccard = get_jaccard_threshold(df_probability)
timewise_clusters = get_timewise_clusters(df_probability)
evolution_dict = track_communities(threshold, timewise_clusters, df_probability, transition_probability, threshold_jaccard)
polished_dict = polish_evolution_dict(evolution_dict)
print(polished_dict)
complete_dict, cluster_data = prepare_cluster_list(polished_dict, community_dict, df.columns, 'de', total_count)
number_of_clusters = get_total_number_clusters(total_count)

df_probability

#unpickled_df = pd.read_pickle("/content/drive/My Drive/Thesis/probabilitydf_new2016.pkl")
unpickled_df

#MAIN
window_length = 3 
total_data = 12 #Give in Months
start = '2016-01-01'
community_dict = {}
partition_dict = {}
for i in range(int(total_data/window_length)):
  community_country_dict = {}
  partition_country_dict = {}
  start_date = datetime.strptime(start.strip(), '%Y-%m-%d')
  start_date = start_date +  relativedelta(months =+ window_length*i)
  #countries = ['at', 'be', 'ch', 'cz', 'de', 'dk', 'es', 'fr', 'gb', 'hu', 'ie', 'it', 'nl', 'pl', 'pt', 'ro', 'se']
  countries = ['de']
  for country in countries:
    G = nx.Graph()
    nodes_path = '/content/drive/My Drive/dataset/PeiLee/nodes_'+country+'.csv'
    edges_path = '/content/drive/My Drive/dataset/PeiLee/'+country+'.csv'
    node_df , edge_df = read_from_path(nodes_path, edges_path)
    #G_master = get_master_graph(nodes_path, edges_path)
    G = read_graph_new (node_df, edge_df, window_length, start_date)
    communities, partition = get_communities (G)
    updated = remove_outliners (communities)
    community_country_dict[country] = updated
    partition_country_dict[country] = partition
  community_dict[i] = community_country_dict
  partition_dict[i] = partition_country_dict
np.save('/content/drive/My Drive/Thesis/Tajuena_clustersupdated_2016.npy', community_dict)
np.save('/content/drive/My Drive/Thesis/out_partitions_2016.npy', partition_dict)

G = nx.Graph()
nodes_path = '/content/drive/My Drive/dataset/Using/de_nodes_T1.csv'
edges_path = '/content/drive/My Drive/dataset/Using/de_T1.csv'
node_df , edge_df = read_from_path(nodes_path, edges_path)
G = read_graph(node_df, edge_df)
communities, partition = get_communities (G)
updated = remove_outliners (communities)
#sorted = sorted(partition.items(), key=operator.itemgetter(1))
print(partition)
#prepareGraphJson(partition, 3)

clusters_evol = np.load('/content/drive/MyDrive/Thesis/out_clusters_2016and2017.npy',allow_pickle='TRUE').item()
quarter = 1
for time, country_head in clusters_evol.items():
  for cluster_number, nodes in country_head['de'].items():
    print(quarter,cluster_number, nodes)
  quarter += 1

#Gexf files generation for whole quarter
clusters_evol = np.load('/content/drive/MyDrive/Thesis/out_clusters_2016and2017.npy',allow_pickle='TRUE').item()
G_master = get_master_graph('/content/drive/My Drive/dataset/PeiLee/nodes_de.csv', '/content/drive/My Drive/dataset/PeiLee/de.csv')
for time, country_head in clusters_evol.items():
  final_graph = nx.Graph()
  for cluster_number, nodes in country_head['de'].items():
    #print(nodes)
    for node in nodes:
      final_graph.add_node(node, attr = {'cluster_number':cluster_number, 'time':time})
    for node1 in nodes:
      for node2 in nodes:
        if G_master.has_edge(node1, node2):
          final_graph.add_edge(node1, node2)
  nx.write_gexf(final_graph, "/content/drive/MyDrive/OutputTajuena/T"+str(time)+"/FullGraph"+str(time)+".gexf")

#Generating Cluster gexf files for each quarter
clusters_evol = np.load('/content/drive/MyDrive/Thesis/out_clusters_2016.npy',allow_pickle='TRUE').item()
G_master = get_master_graph('/content/drive/My Drive/dataset/PeiLee/nodes_de.csv', '/content/drive/My Drive/dataset/PeiLee/de.csv')
for time, country_head in clusters_evol.items():
  for cluster_number, nodes in country_head['de'].items():
    cluster_graph = nx.Graph()
    for node in nodes:
      cluster_graph.add_node(node, attr = {'cluster_number':cluster_number, 'time':time})
    for node1 in nodes:
      for node2 in nodes:
        if G_master.has_edge(node1, node2):
          cluster_graph.add_edge(node1, node2)
    nx.write_gexf(cluster_graph, "/content/drive/MyDrive/OutputTajuena/T"+str(time)+"/Cluster"+str(cluster_number)+".gexf")